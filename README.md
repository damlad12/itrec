Instruction-Tuned and Resource-Efficient Top-k Recommendation Framework using TALLRec, LoRA, and RecRanker (ITREK)

Damla Duendar, Mohammad Merati, Karl Naba, Stan K. Lee
Boston University, ECE 

1. Abstract
ITREK is an integrated framework for a top-k recommender system using TALLRec's lightweight instruction-based tuning approach and RecRanker’s top-k ranking optimization pipeline to improve recommendation accuracy and resource efficiency. Our preliminary experimental results demonstrate improvement in recommendation accuracy compared to baseline approaches. 

2. Introduction
The exponential growth of e-commerce platforms necessitates personalized recommendation systems.  Recent advances in Large Language Models (LLMs) have yielded impressive performance in various natural language processing tasks, prompting research into their potential for recommendation.  TALLRec and RecRanker are two frameworks that use instruction tuning to fine-tune LLMs for recommendation tasks. TALLRec is tailored for the binary classification of the users’ liked and disliked items, whereas RecRanker is for top-k recommendation tasks. 
A core challenge for large-scale recommendation is the computational cost associated with training. TALLRec addresses this by leveraging LoRA (Low-Rank Adaptation of Large Language Models), enabling the fine-tuning of LLaMA on a single NVIDIA RTX 3090 (24GB) GPU. In contrast, RecRanker requires a more extensive setup—16 NVIDIA A800 80GB GPUs for training and a single A800 80GB GPU for inference. Integrating the two systems allows us to combine the resource-efficient LoRA tuning of TALLRec with RecRanker’s advanced ranking pipeline. This synergy maximizes recommendation accuracy while minimizing computational overhead.
Our project improves recommendation accuracy and resource efficiency by:
Optimizing the ranking precision of the TALLRec framework using RecRanker’s pipeline for generating instruction tuning samples. RecRanker’s sample generation pipeline consists of adaptive user sampling, candidate item selection, and prompt construction. RecRanker addresses the performance variations of LLMs across different ranking tasks by employing a hybrid ranking method. This approach involves assigning a utility score for items in each of the LLM responses generated by the different ranking methods: pointwise, pairwise, and listwise ranking.
Utilizing TALLRec for instruction-based and lightweight fine-tuning of Llama-7B using LoRA.


3. Methodology
3.1 Framework Integration
ITREK integrates three primary frameworks: TALLRec, RecRanker, and LoRA. TALLRec serves as a framework for instruction-based tuning of the LLaMA-7B model, utilizing LoRA to reduce the number of trainable parameters and thus the overall GPU footprint. RecRanker supplies the hybrid ranking prompt-generation pipeline, which uses pointwise, pairwise, and listwise ranking outputs from an LLM to compute final item utility scores. 

3.2 Implementation Details
We employ LLaMA (7B) as the backbone LLM. In the training phase, we adopt a learning rate of 1e-5, a context length of 1024, a batch size of 4, and gradient accumulation steps of 2. A cosine scheduler is used, along with 50 warm-up steps, for a total of 6000 training steps. The model is trained in 8-bit precision with LoRA-based adaptations on a single NVIDIA A100 GPU, leveraging parameter-efficient fine-tuning. We apply LoRA with a rank of 16, scaling factor of 16, and a dropout rate of 0.05. The LoRA modules target the q_proj, k_proj, v_proj, and o_proj layers.
For inference, we employ the vLLM framework. We load the LoRA-adapted weights on the base LLaMA (7B) model, then process the test data via a batched pipeline to generate top-k recommendation outputs. We configure the sampling parameters with a temperature of 0.1, top-k set to 10, top-p set to 0.1, and a maximum of 300 tokens per response. Consistent with the RecRanker paper, we set  1 = 2 = 3 = 13 for all experiments. C is set to 0.92  and C1, C2, and C3 are set to 0.05, 0.5, and 0.025 respectively.We execute the inference process on a single NVIDIA A100 GPU. 



4. Dataset and Metrics
4.1 Dataset
ML-100K Dataset:
943 users, 1,682 items, 100,000 ratings.
Train-validation-test split: 80%-10%-10% (user-specific).
4.2 Metrics
Hit Ratio (HR)
Normalized Discounted Cumulative Gain (NDCG)


5.Results

### Table 1. ITREK top-k Ranking results using Matrix Factorization as the baseline:

| Backbone | Method            | H@3↑  | N@3↑  | H@5↑   | N@5↑   |
|----------|-------------------|--------|-------|--------|--------|
| Base     |                   | 0.04639 | 0.03272 | 0.0712 | 0.04298 |
| MF       | ITREKpairwise     | 0.071  | 0.0525 | 0.1085 | 0.068   |
| MF       | ITREKpointwise    | 0.0762 | 0.0551 | 0.123  | 0.0742  |
| MF       | ITREKlistwise     | 0.0727 | 0.0522 | 0.1288 | 0.0752  |
| MF       | ITREKhybrid       | 0.0768 | 0.0551 | 0.1169 | 0.0715  |

### Improvement Over Base
| Method                | H@3↑   | N@3↑   | H@5↑   | N@5↑   |
|-----------------------|--------|--------|--------|--------|
| ITREKhybrid           | 68.8%  | 69.5%  | 69.6%  | 70.2%  |
| RecRankerhybrid       | 51.65% | 57.85% | 33.19% | 44.52% |

Using the same conventional recommender (Matrix Factorization) as the backbone, ITREK achieves greater improvement compared to RecRanker. The percent improvements for RecRankerhybrid were given in their paper [2].


7. Conclusions
The results show that, when using the same baseline recommender, ITREK obtains greater improved performance compared to RecRanker. Integrating TALLRec, LoRA, and RecRanker into a unified top-k recommendation framework shows promising potential for balancing accuracy and computational cost. 

8. References
Bao, K., Zhang, J., Lin, X., et al. "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Models with Recommendation." ACM SIGIR, 2023.
Luo, S., et al. "RecRanker: Instruction Tuning Large Language Model as Ranker for Top-K Recommendation." arXiv, 2024.
Hu, E.J., et al. "LoRA: Low-Rank Adaptation of Large Language Models." arXiv, 2021.





